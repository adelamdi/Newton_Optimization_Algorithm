{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e8a683-ccd2-48d8-b005-00b2e8aec7f4",
   "metadata": {},
   "source": [
    "# Introduction to Newton's Optimization: A Practical Example\n",
    "## Authors: Fariman AA, Kiani M\n",
    "## May 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5d606-f8ce-45d2-9766-b4a6ec1784d1",
   "metadata": {},
   "source": [
    "We begin by considering the following function:\n",
    "\n",
    "$y = 3 sin(2t + π/6)$\n",
    "\n",
    "However, in a more general scenario, we may not have access to the specific constants defining the function. Instead, we might encounter a function of the form:\n",
    "\n",
    "$y = A sin(Wt + Z)$\n",
    "\n",
    "where A, W, and Z are unknown parameters. Our objective is to determine the values of A, W, and Z that would yield a solution similar to the first function using Newton's method for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6594154a-0bf8-4c52-9fdc-2bcfc3d8174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991ba18d-c6bb-42c8-b433-9648e7685565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(t, params):\n",
    "    A, W, Z = params\n",
    "    return A * np.sin(W * t + Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bc1d1-f886-4a6c-ad3f-0220bab24c5e",
   "metadata": {},
   "source": [
    "We know that derivative of y with respect to A is: \n",
    "\n",
    "$sin(W*t + Z)$.\n",
    "\n",
    "Derivative of y with respect to W is:\n",
    "\n",
    "$A * t * cos(W*t + Z)$\n",
    "\n",
    "and derivative of y with respect to Z is:\n",
    "\n",
    "$A * cos(W*t + Z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a172b2-239b-4d8f-8f01-30dbef8440f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcDeriv(t, params):\n",
    "    A, W, Z = params\n",
    "    dA = np.sin(W * t + Z)\n",
    "    dW = A * t * np.cos(W * t + Z)\n",
    "    dZ = A * np.cos(W * t + Z)\n",
    "    return np.array([dA, dW, dZ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df4e4c-52cd-47ed-9264-761334cb6835",
   "metadata": {},
   "source": [
    "In this context, we employ the Mean Squared Error (MSE) loss function as our optimization criterion. \n",
    "\n",
    "$MSE = 1 / n {‎‎\\sum}_{n=1}^{n} (y_i - y\\hat{} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019c9983-f493-4e3d-81cf-581945d962ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunction(params):\n",
    "    return np.sum((y_data - func(t_data, params)) ** 2)/ len(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c549574-e80c-4882-a66c-3311f317bfab",
   "metadata": {},
   "source": [
    "Now we define a function for gradient of the MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41dfe018-e317-433e-9b7f-2aa67b084ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunctionGradient(params):\n",
    "    error = y_data - func(t_data, params)\n",
    "    deriv = funcDeriv(t_data, params)\n",
    "    grad = -2 * np.dot(deriv, error)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d2a5a-ea5b-40a2-b8c4-346448874530",
   "metadata": {},
   "source": [
    "Now lets implement the Newton optimization method for finding optimal parameter. Within the function, an iterative process is employed and at each iteration, the gradient of the loss function is computed and an update step is performed using the Newton formula, which involves the inversion of the Hessian matrix (the second derivative matrix of the loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5abc3b-2f90-4baa-a2bd-84e2a7393b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton optimization algorithm\n",
    "def newtonMethod(func, lostFunction, lossFunctionGradient, params_initial, max_iter=100, tol=1e-6):\n",
    "    params = params_initial\n",
    "    for _ in range(max_iter):\n",
    "        grad = lossFunctionGradient(params)\n",
    "        grad = grad.reshape(-1, 1)  # Reshape to ensure at least 2 dimensions\n",
    "        # Calculating Hessian matrix inverse and update params using newton rule, we call flatten to create 1D array\n",
    "        params_new = params - np.linalg.inv(np.dot(grad, grad.T)).dot(grad).flatten()\n",
    "        # Convergence checking\n",
    "        if np.linalg.norm(params_new - params) < tol:\n",
    "            break\n",
    "        params = params_new\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b8290-d3af-4b01-88cf-8f38ddc2f2e6",
   "metadata": {},
   "source": [
    "We initiate the solution finding part by generating a set of data points from the first function. To achieve this, we discretize the given interval into 100 equally spaced points and evaluate the function at each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4681c251-5d90-441e-9cea-0414fbcaedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_initial = np.array([1, 1, 1])  # You can adjust the initial guess if you have some insight\n",
    "t_data = np.linspace(0, 3*np.pi, 100)\n",
    "y_data = func(t_data, [3, 2, np.pi/6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1a321-532b-46d7-94e7-e900c9cd4c38",
   "metadata": {},
   "source": [
    "We now proceed to determine the optimal values of A, W, and Z that minimize the MSE loss function using Newton algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d82d04-5224-46e4-af2d-eb7caf01b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized answers:\n",
      "A = 2.733006162444647\n",
      "W = 0.9747863738253851\n",
      "Z = 1.2223306215771568\n"
     ]
    }
   ],
   "source": [
    "params_optimized = newtonMethod(func, lossFunction, lossFunctionGradient, params_initial)\n",
    "print(\"Optimized answers:\")\n",
    "print(\"A =\", params_optimized[0])\n",
    "print(\"W =\", params_optimized[1])\n",
    "print(\"Z =\", params_optimized[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301f1e9-8cfe-4e9e-9d03-3ae777fb5333",
   "metadata": {},
   "source": [
    "As we can saw in the results, the Newton optimization method has found solutions for our given problem. It is pertinent to note that the effectiveness of the Newton optimization method, is contingent upon the selection of initial values for the parameters.\n",
    "\n",
    "Best regards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
